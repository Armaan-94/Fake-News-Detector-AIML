{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e73399-3d64-47d5-a74f-3eaa45c764fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Armaan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELFake_Dataset Loaded:\n",
      "                                               title  \\\n",
      "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
      "2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
      "3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
      "4  SATAN 2: Russia unvelis an image of its terrif...   \n",
      "5  About Time! Christian Group Sues Amazon and SP...   \n",
      "\n",
      "                                                text  label  \n",
      "0  No comment is expected from Barack Obama Membe...      1  \n",
      "2   Now, most of the demonstrators gathered last ...      1  \n",
      "3  A dozen politically active pastors came here f...      0  \n",
      "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  \n",
      "5  All we can say on this one is it s about time ...      1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 71537 entries, 0 to 72133\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   71537 non-null  object\n",
      " 1   text    71537 non-null  object\n",
      " 2   label   71537 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.2+ MB\n",
      "None\n",
      "\n",
      "FA-KES-Dataset Loaded:\n",
      "                                               title  \\\n",
      "0  Syria attack symptoms consistent with nerve ag...   \n",
      "1  Homs governor says U.S. attack caused deaths b...   \n",
      "2    Death toll from Aleppo bomb attack at least 112   \n",
      "3        Aleppo bomb blast kills six Syrian state TV   \n",
      "4  29 Syria Rebels Dead in Fighting for Key Alepp...   \n",
      "\n",
      "                                                text  label  \n",
      "0  Wed 05 Apr 2017 Syria attack symptoms consiste...      0  \n",
      "1  Fri 07 Apr 2017 at 0914 Homs governor says U.S...      0  \n",
      "2  Sun 16 Apr 2017 Death toll from Aleppo bomb at...      0  \n",
      "3  Wed 19 Apr 2017 Aleppo bomb blast kills six Sy...      0  \n",
      "4  Sun 10 Jul 2016 29 Syria Rebels Dead in Fighti...      0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 804 entries, 0 to 803\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   804 non-null    object\n",
      " 1   text    804 non-null    object\n",
      " 2   label   804 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 19.0+ KB\n",
      "None\n",
      "\n",
      "Combined Dataset:\n",
      "                                               title  \\\n",
      "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
      "1  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
      "2  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
      "3  SATAN 2: Russia unvelis an image of its terrif...   \n",
      "4  About Time! Christian Group Sues Amazon and SP...   \n",
      "\n",
      "                                                text  label  \\\n",
      "0  No comment is expected from Barack Obama Membe...      1   \n",
      "1   Now, most of the demonstrators gathered last ...      1   \n",
      "2  A dozen politically active pastors came here f...      0   \n",
      "3  The RS-28 Sarmat missile, dubbed Satan 2, will...      1   \n",
      "4  All we can say on this one is it s about time ...      1   \n",
      "\n",
      "                                             content  \n",
      "0  law enforcement high alert following threats c...  \n",
      "1  unbelievable obamas attorney general says char...  \n",
      "2  bobby jindal raised hindu uses story christian...  \n",
      "3  satan russia unvelis image terrifying new supe...  \n",
      "4  time christian group sues amazon splc designat...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72341 entries, 0 to 72340\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    72341 non-null  object\n",
      " 1   text     72341 non-null  object\n",
      " 2   label    72341 non-null  int64 \n",
      " 3   content  72341 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.2+ MB\n",
      "None\n",
      "\n",
      "Preprocessed Content Sample:\n",
      "0    law enforcement high alert following threats c...\n",
      "1    unbelievable obamas attorney general says char...\n",
      "2    bobby jindal raised hindu uses story christian...\n",
      "3    satan russia unvelis image terrifying new supe...\n",
      "4    time christian group sues amazon splc designat...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the WELFake_Dataset\n",
    "file_path1 = r\"C:\\Users\\Armaan\\OneDrive\\Documents\\Projects\\FakeNewsDetector\\WELFake_Dataset.csv\"\n",
    "data1 = pd.read_csv(file_path1)\n",
    "\n",
    "# Load the FA-KES-Dataset with encoding specified\n",
    "file_path2 = r\"C:\\Users\\Armaan\\OneDrive\\Documents\\Projects\\FakeNewsDetector\\FA-KES-Dataset.csv\"\n",
    "data2 = pd.read_csv(file_path2, encoding='ISO-8859-1')  # Try 'utf-8' if this doesn't work\n",
    "\n",
    "# Standardize the FA-KES-Dataset column names to match WELFake_Dataset\n",
    "data2 = data2.rename(columns={\n",
    "    'article_title': 'title',\n",
    "    'article_content': 'text',\n",
    "    'labels': 'label'\n",
    "})\n",
    "\n",
    "# Keep only relevant columns\n",
    "data1 = data1[['title', 'text', 'label']].dropna()\n",
    "data2 = data2[['title', 'text', 'label']].dropna()\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "# Combine 'title' and 'text' into a single field\n",
    "combined_data['content'] = combined_data['title'] + \" \" + combined_data['text']\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the combined data\n",
    "combined_data['content'] = combined_data['content'].apply(preprocess_text)\n",
    "\n",
    "# Define features and labels\n",
    "X = combined_data['content']\n",
    "y = combined_data['label']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# After loading data1\n",
    "print(\"WELFake_Dataset Loaded:\")\n",
    "print(data1.head())\n",
    "print(data1.info())\n",
    "\n",
    "# After loading data2\n",
    "print(\"\\nFA-KES-Dataset Loaded:\")\n",
    "print(data2.head())\n",
    "print(data2.info())\n",
    "\n",
    "# After combining datasets\n",
    "print(\"\\nCombined Dataset:\")\n",
    "print(combined_data.head())\n",
    "print(combined_data.info())\n",
    "\n",
    "# After preprocessing\n",
    "print(\"\\nPreprocessed Content Sample:\")\n",
    "print(combined_data['content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d39c17-f419-4de3-9e5c-49539c155285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on WELFake Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84      7006\n",
      "           1       0.84      0.87      0.85      7302\n",
      "\n",
      "    accuracy                           0.85     14308\n",
      "   macro avg       0.85      0.85      0.85     14308\n",
      "weighted avg       0.85      0.85      0.85     14308\n",
      "\n",
      "Accuracy on WELFake: 0.848406485882024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorize the WELFake dataset\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Split WELFake dataset into training and testing sets\n",
    "X_train_welfake, X_test_welfake, y_train_welfake, y_test_welfake = train_test_split(\n",
    "    data1['text'].apply(preprocess_text),\n",
    "    data1['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data1['label']\n",
    ")\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "X_train_welfake_tfidf = vectorizer.fit_transform(X_train_welfake)\n",
    "X_test_welfake_tfidf = vectorizer.transform(X_test_welfake)\n",
    "\n",
    "# Initialize Multinomial Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model on the WELFake dataset\n",
    "model.fit(X_train_welfake_tfidf, y_train_welfake)\n",
    "\n",
    "# Evaluate the model on the WELFake test set\n",
    "y_pred_welfake = model.predict(X_test_welfake_tfidf)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation on WELFake Dataset:\")\n",
    "print(classification_report(y_test_welfake, y_pred_welfake))\n",
    "print(\"Accuracy on WELFake:\", accuracy_score(y_test_welfake, y_pred_welfake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be289d7b-986b-4045-9f2a-2562cb2a4d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on WELFake Dataset (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84      7006\n",
      "           1       0.84      0.87      0.85      7302\n",
      "\n",
      "    accuracy                           0.85     14308\n",
      "   macro avg       0.85      0.85      0.85     14308\n",
      "weighted avg       0.85      0.85      0.85     14308\n",
      "\n",
      "Accuracy on WELFake Test Set: 0.848406485882024\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the WELFake test set\n",
    "y_pred_welfake = model.predict(X_test_welfake_tfidf)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation on WELFake Dataset (Test Set):\")\n",
    "print(classification_report(y_test_welfake, y_pred_welfake))\n",
    "print(\"Accuracy on WELFake Test Set:\", accuracy_score(y_test_welfake, y_pred_welfake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b92ac99a-6628-45da-8310-4348ebb56fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1:\n",
      "Content: The government has announced a new plan to combat climate change. The initiative will focus on reducing carbon emissions and promoting renewable energy sources.\n",
      "Prediction: Real\n",
      "--------------------------------------------------\n",
      "Article 2:\n",
      "Content: Scientists have discovered a new species of dinosaur in Antarctica, which could shed light on the region's prehistoric ecosystem.\n",
      "Prediction: Fake\n",
      "--------------------------------------------------\n",
      "Article 3:\n",
      "Content: Celebrity gossip: A famous singer is reportedly in a relationship with a fellow artist. Fans are excited about the new couple.\n",
      "Prediction: Fake\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example new news articles for testing\n",
    "new_articles = [\n",
    "    \"The government has announced a new plan to combat climate change. The initiative will focus on reducing carbon emissions and promoting renewable energy sources.\",\n",
    "    \"Scientists have discovered a new species of dinosaur in Antarctica, which could shed light on the region's prehistoric ecosystem.\",\n",
    "    \"Celebrity gossip: A famous singer is reportedly in a relationship with a fellow artist. Fans are excited about the new couple.\"\n",
    "]\n",
    "\n",
    "# Preprocess the new articles (apply the same preprocessing as before)\n",
    "new_articles_preprocessed = [preprocess_text(article) for article in new_articles]\n",
    "\n",
    "# Vectorize the new articles using the same TF-IDF vectorizer\n",
    "new_articles_tfidf = vectorizer.transform(new_articles_preprocessed)\n",
    "\n",
    "# Make predictions on the new articles\n",
    "predictions = model.predict(new_articles_tfidf)\n",
    "\n",
    "# Output the predictions (flip the label interpretation if necessary)\n",
    "for i, article in enumerate(new_articles):\n",
    "    print(f\"Article {i+1}:\")\n",
    "    print(f\"Content: {article}\")\n",
    "    print(f\"Prediction: {'Fake' if predictions[i] == 1 else 'Real'}\")  # Adjusted for correct output\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c163f7f0-ce22-4336-b356-ddf0f8178ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'fake_news_model_welfake.pkl')\n",
    "\n",
    "# Save the vectorizer to a file (since you'll need to use the same vectorizer for future predictions)\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer_welfake.pkl')\n",
    "\n",
    "print(\"Model and Vectorizer saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
